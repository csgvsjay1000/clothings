
# ES #

- Elasticsearch是如何实现Master选举的？
- Elasticsearch中的节点（比如共20个），其中的10个选了一个master，另外10个选了另一个master，怎么办？
- 客户端在和集群连接时，如何选择特定的节点执行请求的？
- 详细描述一下Elasticsearch索引文档的过程。
- 详细描述一下Elasticsearch更新和删除文档的过程。
- 详细描述一下Elasticsearch搜索的过程。
- 在Elasticsearch中，是怎么根据一个词找到对应的倒排索引的？
- Elasticsearch在部署时，对Linux的设置有哪些优化方法？
- 对于GC方面，在使用Elasticsearch时要注意什么？
- Elasticsearch对于大数据量（上亿量级）的聚合如何实现？
- 在并发情况下，Elasticsearch如果保证读写一致？
- 如何监控Elasticsearch集群状态？
- 介绍下你们电商搜索的整体技术架构。
- 介绍一下你们的个性化搜索方案？
- 是否了解字典树？
- 拼写纠错是如何实现的？

## Elasticsearch是如何实现Master选举的？##

- es选master主要是由ZenDiscovery模块负责，主要包含Ping和Uncast，Ping是指发送RPC请求，节点之间通过发送RPC来发现彼此。Uncast单播模块，主要是节点配置了主机列表，以控制哪些节点需要Ping。
- 对所有可以成为master节点的node.id排序，每次选举每个节点，都把自己知道的节点拍一次序，然后选出第一个节点，暂且认为他是master节点。
- 如果对某个节点的投票数达到（master/2 + 1）,并且该节点也投自己为master节点，则该节点成为master节点，否则重新选取，直到成功为止。
- 补充，master节点职责主要包括：集群、节点和索引的管理，不负责文档级别的管理，data节点可以关闭http的功能。主节点功能包括集群相关操作。
-- 集群相关操作：创建和删除索引，跟踪哪些节点是集群的一部分，决定哪些分片分配给相关节点。

## Elasticsearch中的节点（比如共20个），其中的10个选了一个master，另外10个选了另一个master，怎么办？ ##

- 当候选主节点数量不小于3时，可以通过配置最少投票通过数(discovery.zen.minimum_master_nodes)来避免脑裂问题。
- 当候选数量为为两个时只能修改惟一的一个为master。

## 客户端在和集群连接时，如何选择特定的节点执行请求的？  ##

- 当使用TansportClient连接一个ES集群时，它并不加入集群中，只是简单的获取集群服务端地址，并以轮询的方式与集群通信。

## 详细描述一下Elasticsearch索引文档的过程  ##

- 请求到达协调节点，默认使用文档ID计算，以便提供合适的分片。公式：shard = hash(ID)%(num_of_primary_shards)
- 当分片接收到来自协调节点的请求后，会将请求写到Memory Buffer, 然后定时(每隔1s)写到FileSystem Cache.这个过程叫refresh。
- 当然在某些情况下，在Memory Buffer和FileSystem Cache里的数据可能会丢失，ES通过TransLog机制保证数据的可靠性。其实现方式是，接收到请求后，先写入到TransLog(写入Memory Buffer成功后)。当FileSystem Cache数据写到磁盘后，TransLog数据会清除掉.这个过程叫flush。
- flush触发机制是定时机制（默认30分钟），或当TransLog文件达到512M时。

- 关于Lucene的Segment段的补充：
-- lucene索引是由多个段组成，段本身是一个功能齐全的倒排索引。
-- 段是不可变的，允许Lucene增量的将新的文档添加到索引中，而不用重头建索引。
-- 对于每一个搜索请求而言，索引中的所有段都会被搜索，并且每个段会消耗CPU、IO和内存。这意味着段的数量越多，搜索性能越低。
-- 为了解决这个问题，ES会合并小段为一个大段，删除旧的小段。


## 详细描述一下Elasticsearch更新和删除文档的过程 ##

- 删除和更新也是写操作，因为ES中文档时不可变的，因此不能被删除或改动，以示其变更。
- 磁盘上的每个段都有一个.del文件，当删除请求发送后，文档并不会真正被删除，而是在.del文件标记为已删除，该文档依然可以匹配查询，只是在结果中会被过滤掉。当段合并时，被标记的文档将不会写入新的段。
- 当新的文档被创建时，ES会为该文档指定一个版本号，当执行更新时，旧的文档在.del被标记为已删除，新版本的文档会被索引到一个新段，旧版本的文档依然会匹配到，只是会在过滤中去除。

## 详细描述一下Elasticsearch搜索的过程 ##

- 搜索被分为两个阶段：Query Then Fetch
- 在初始查询阶段，查询会广播到索引的每个分片的副本(包括主分片和副分片),每个分片在本地执行搜索并构建一个匹配文档，大小为from+size的优先队列。PS:在搜索时会从FileSystem Cache里获取数据，但是有些数据还在Memory Buffer里，所以说搜索时近实时的。
- 每个分片返回各自优先队列所有文档ID和排序值给协调节点，它合并这些值到自己的优先队列，来产生一个全局的优先列表。
- 接下来就是取数据阶段，协调节点分析出哪些文档需要被取回，并向相关分片提交多个GET请求，每个分片加载并丰富文档，接着返回文档给协调节点，一旦所有文档都被取回了，协调节点就返回数据给客户端。

## Elasticsearch在部署时，对Linux的设置有哪些优化方法

- 内存，64G内存是比较理想的，32G和16G也是很常见的，少于8G会适得其反
- 在更多内核和更快CPUs的选择上，更倾向于选择更多内核
- 如果担负的其SSD，基于SSD节点的实例，索引和搜索性能都有很大的提高
- 绝对避免集群跨越大的地理距离
- 确保应用服务器JVM和ES的JVM保持一致，因为ES有用到java本地序列化
- 通过设置gateway.recover_after_nodes, gateway.expected_nodes, gateway.recover_after_time 可以避免在集群重启时避免过多的分片交换，这可能让数据恢复从几小时变为几分钟。
- ES默认使用单播发现，这样可以避免节点无意中加入集群，只有运行在同一台机器上的节点才会自动组成集群。最好使用单播代替组播。
- 不要随意修改垃圾回收器和线程池大小
- 把你的内存一半给Lucene，但不要超过32G,通过 ES_HEAP_SIZE 设置。
- ES 大量使用了文件，同时各个节点之间通信也大量使用了套接字，所以增大文件描述符值。

补充：索引阶段提升性能的方法

- 使用批量请求并调整其大小：每次批量数据在5-15MB是个不错的选择
- 存储：使用SSD
- 段与合并：ES默认是20MB/s，对机械磁盘是不错的选择，如果使用了SSD，可以考虑提高到100-200MB/s，另外还可以增加index.translog.flush_threshold_size，从默认的512MB到1G设置。可以在一次触发时累计更大的段。
- 如果在大批量的导入时，可以通过设置index.num_of_replicas=0来关闭副本

## 对于GC方面，在使用Elasticsearch时要注意什么？

- 倒排词典的索引需要常驻内存，无法GC。所以需要监控 data node 上 Segment memory增长趋势
- 各类缓存：field cache、filter cache、indexing cache、bulk queue需要合理设置并监控，监控heap是否够用。也就是各类缓存占满时，还有空间去分配给其他任务吗。
- 避免返回大量结果集的搜索和聚合，确实需要拉取大量数据时，考虑使用scroll api & scan
- Cluster state 驻留内存无法水平扩展，超大规模的集群可以考虑拆分成多个集群通过tribe node 连接
- 要对heap做持续性的监控

## 在并发情况下，ES怎么保证读写一致性

- 通过乐观锁实现并发控制，避免旧的数据覆盖新的数据
- 写一致性级别有：quorum/one/all, 默认为quorum, 即当大多数分片（包括主分片和副分片）可用时才允许写操作，但即使大多数可用时，也可能存在因为网络原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建, 若quorum不足时，会等待，默认1分钟超时将返回写入失败。
- 对于读操作，replica为sync（默认），这使得操作在主副分片都完成时才返回，如果设置为async，可以通过设置搜索参数_preference为primary来查询主片，确保文档最新版本。


